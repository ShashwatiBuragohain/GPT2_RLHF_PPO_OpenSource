# RLHF_PPO_Tuned_GPT2
**RLHF_PPO_Tuned_GPT2** fine-tunes GPT-2 using **Reinforcement Learning from Human Feedback (RLHF)** and **Proximal Policy Optimization (PPO)** to generate responses aligned with human preferences. This project enhances response helpfulness, truthfulness, and harmlessness through reward-based learning, significantly outperforming the vanilla GPT-2 model.
The project focuses on alignment: teaching language models not just to sound good, but to behave responsibly when answering both normal and unsafe prompts.
## Fine-Tuning Process  
#### Reward Model Training:  

- A **reward model** was trained using human feedback from the yitingxie/rlhf-reward-datasets to assign positive or negative feedback based on response quality. 
- The dataset was filtered, limiting prompts and responses to under 500 tokens for efficiency during training.
  
#### PPO Tuning:  
  
- The model was further refined with PPO, ensuring responses are aligned with human feedback by optimizing behavior based on the reward scores generated by the reward model.
- **KL divergence** and a conservative learning rate were used to maintain stability during training, improving response safety and relevance.
  
## Model Architecture  

- **Base Model**: GPT-2 Medium.  
- **Reward Model**: Predicts feedback scores (positive or negative) based on the helpfulness and appropriateness of responses.  
- **PPO Head**: Enhances the GPT-2 model by introducing reinforcement learning to align model outputs with human feedback.

## Inference and Evaluation  

During inference, responses were generated using both the **vanilla GPT-2** and the **PPO-tuned GPT-2** models. Responses were compared using criteria like:

1. **Helpfulness**: Whether the response answered the question effectively.  
2. **Truthfulness**: The factual accuracy of the response.  
3. **Harmlessness**: Avoiding misleading or harmful content.  
  
### Output Analysis:
A total of 36 prompts were used for evaluation. The responses were reviewed and judged based on their adherence to the three criteria. It was done using ChatGPT4o (did not use API to keep cost down and as data was small) - Check out this link for **detailed analysis**: _**https://chatgpt.com/share/66eb698e-d66c-8004-9318-bf354ce88fa2**_

#### Results:

- vanilla_responses: 9 better responses  
- ppo_responses: 23 better responses  
- Neither: 4 responses were equally poor.
  
The PPO-tuned GPT-2 model significantly outperformed the vanilla model, providing more relevant, less repetitive, and safer answers. PPO fine-tuning helped the model handle sensitive topics more responsibly, demonstrating greater care in avoiding harmful or misleading outputs.  

### Detailed Findings:
 
- **PPO-tuned responses** were generally more coherent and aligned better with human feedback, making them more useful in conversational contexts.  
- **Vanilla GPT-2 responses** often lacked the depth and safety precautions observed in the PPO-tuned model, frequently generating repetitive or less informative answers.
### Evaluation & Results

To measure how well the models align with human preferences, we evaluated vanilla GPT-2 and a PPO-style aligned model using a safety-aware, multi-metric evaluation setup inspired by RLHF principles.
Instead of relying only on output length or manual inspection, we used automated metrics that capture real alignment behavior.
- Metrics Used
Each response was scored using a weighted combination of:
- Helpfulness / Relevance: Semantic similarity between the prompt and response using sentence embeddings
- Safety / Harmlessness: Explicit reward for refusing unsafe or illegal requests (e.g., shoplifting, dark web)
-Fluency: Penalizes repetition and low-quality text
Scores were aggregated per prompt, and the higher-scoring response was counted as a “win.”
  
## Overall Summary
  
The **RLHF PPO-tuned GPT-2** model offers significant improvements over the **vanilla GPT-2**, particularly in relevance, safety, and overall helpfulness. By combining RLHF and PPO, the model aligns more closely with human preferences, making it more suitable for real-world applications that require trustworthy and non-harmful outputs.

## Kaggle Notebook Link:  
https://www.kaggle.com/code/karthiksundaram123/rlhf-ppo-tuned




